{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "demoTrainAndTest_Inception.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srPHiZhPjbCF"
      },
      "source": [
        "Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9fmd4KnymJr",
        "outputId": "26261703-d5cc-43c6-dd0c-2f644ae40b3c"
      },
      "source": [
        "# Provides similar functionality to ImageDataGenerators for videos\n",
        "!pip install keras-video-generators"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-video-generators\n",
            "  Downloading https://files.pythonhosted.org/packages/d9/98/ff550be6084b0537f1340783a6850a2f2b62b87273472a56c17ed84bcdb3/keras-video-generators-1.0.14.tar.gz\n",
            "Requirement already satisfied: keras>=2 in /usr/local/lib/python3.7/dist-packages (from keras-video-generators) (2.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-video-generators) (1.19.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from keras-video-generators) (4.1.2.30)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from keras-video-generators) (3.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras>=2->keras-video-generators) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras>=2->keras-video-generators) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras>=2->keras-video-generators) (1.4.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->keras-video-generators) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->keras-video-generators) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->keras-video-generators) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->keras-video-generators) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras>=2->keras-video-generators) (1.15.0)\n",
            "Building wheels for collected packages: keras-video-generators\n",
            "  Building wheel for keras-video-generators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-video-generators: filename=keras_video_generators-1.0.14-cp37-none-any.whl size=12884 sha256=99ebb18b46c379bd66a3524a61f60d87a83b186bbe25211fe3dc312596df2b34\n",
            "  Stored in directory: /root/.cache/pip/wheels/20/b7/76/8674d46fc4777c09e5aa7b065d4e356d90f12ec409a6144bbb\n",
            "Successfully built keras-video-generators\n",
            "Installing collected packages: keras-video-generators\n",
            "Successfully installed keras-video-generators-1.0.14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2nHw4R7yp3C",
        "outputId": "355976c9-af7a-49d5-e9ed-5d35a98e2ba6"
      },
      "source": [
        "# Please email me at nini16@tamu.edu if you do not ave access to the google drive.\n",
        "# Permissions should have been granted but if not please email me!\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "# drive.flush_and_unmount()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UA_Vh5HTzPjH"
      },
      "source": [
        "# training data.\n",
        "# Please ensure the file is present before running.\n",
        "!unzip -q \"/content/drive/MyDrive/CSCE636/v10/dataset_v10.zip\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrBsbdoWyXgq"
      },
      "source": [
        "import keras\n",
        "from keras.regularizers import l2\n",
        "from keras.preprocessing.image import load_img\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from keras.layers import Conv2D, BatchNormalization, \\\n",
        "    MaxPool2D, GlobalMaxPool2D, Dense, Dropout, Bidirectional\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras_video import VideoFrameGenerator\n",
        "\n",
        "from keras.layers import TimeDistributed, GRU, Dense, Dropout, LSTM\n",
        "\n",
        "from keras.models import load_model, Model\n",
        "\n",
        "from keras.applications import InceptionV3, DenseNet121\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import math"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7Xx0DXOzJ30"
      },
      "source": [
        "# All frames should be resized\n",
        "# Please select a batch-size that divides the number of samples!\n",
        "# THE DATASET WILL PROBABLY BE INCREASED FOR THE NEXT SUBMISSION SO\n",
        "# BE SURE TO ADJUST THE BATCH SIZE!!\n",
        "\n",
        "img_shape = (224, 224)\n",
        "BS = 22"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kS-o0BZzJ7v",
        "outputId": "a3f14916-a30c-4b55-f88e-26e92842f608"
      },
      "source": [
        "# Apply image augmentation to each frame\n",
        "# Please confirm that this directory is present before running\n",
        "\n",
        "vid_gen = VideoFrameGenerator(\n",
        "    glob_pattern=r\"/content/dataset_v10/{classname}/*\",\n",
        "    nb_frames=20, \n",
        "    shuffle=True,\n",
        "    split_val=0.1155,\n",
        "    batch_size=BS,\n",
        "    target_shape=img_shape,\n",
        "    nb_channel=3,\n",
        "    transformation=ImageDataGenerator(rescale=1./255,\n",
        "                                      samplewise_center=True,\n",
        "                                      # rotation_range=30,\n",
        "                                      # width_shift_range=0.1,\n",
        "                                      # height_shift_range=0.1,\n",
        "                                      # shear_range=0.1,\n",
        "                                      # zoom_range=0.1,\n",
        "                                      # horizontal_flip=True,\n",
        "                                      fill_mode=\"nearest\"),\n",
        "    use_frame_cache=False)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "class watering_plants, validation count: 55, train count: 429\n",
            "class z_miscellaneous, validation count: 55, train count: 429\n",
            "Total data: 2 classes for 858 files for train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqbQiJKp784D",
        "outputId": "658ab04a-0ada-41ad-f485-0ef0dd3c6608"
      },
      "source": [
        "validation_gen = vid_gen.get_validation_generator()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total data: 2 classes for 110 files for validation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxiE9U_YzJ_2"
      },
      "source": [
        "# model structure for Feature Extractor\n",
        "def build_convnet_3(shape=(224, 224, 3)):\n",
        "    incnet = InceptionV3(include_top=False, weights='imagenet', input_shape=shape)\n",
        "\n",
        "    train = False\n",
        "    for layer in incnet.layers:\n",
        "        layer.trainable = train\n",
        "#         if layer.name == \"conv5_block3_2_relu\":\n",
        "#             train = True\n",
        "    \n",
        "    globMaxpool = GlobalMaxPool2D()(incnet.output)\n",
        "    model = Model(inputs=incnet.input, outputs=globMaxpool)\n",
        "    return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RlyaxyBzMf6"
      },
      "source": [
        "def action_model(shape=(20,) + img_shape + (3,)):\n",
        "    # Create our feature extractor convnet with img_shape input shape\n",
        "    convnet = build_convnet_3()\n",
        "    \n",
        "    # then create our final model\n",
        "    model = keras.Sequential()\n",
        "    # add the convnet with img_shape shape\n",
        "    model.add(TimeDistributed(convnet, input_shape=shape))\n",
        "    # add GRU\n",
        "    model.add(Bidirectional(LSTM(64)))\n",
        "    # and finally, we make a decision network\n",
        "    model.add(Dense(1024, activation='relu', kernel_regularizer=keras.regularizers.l2(l2=0.01)))\n",
        "    model.add(Dropout(.5))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcVhpRcs3iLk",
        "outputId": "dc644530-6694-4e2a-d6a6-f65df1d53e4b"
      },
      "source": [
        "# instantiate and compile model\n",
        "model = action_model()\n",
        "optimizer = keras.optimizers.Adam(0.0005) #0.0005\n",
        "model.compile(\n",
        "    optimizer,\n",
        "    'categorical_crossentropy',\n",
        "    metrics=['acc']\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "time_distributed (TimeDistri (None, 20, 2048)          21802784  \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 128)               1081856   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1024)              132096    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 2050      \n",
            "=================================================================\n",
            "Total params: 23,018,786\n",
            "Trainable params: 1,216,002\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjuNkqG47uXR",
        "scrolled": true,
        "outputId": "e3a8a33f-2bdc-40e6-ebfe-0e7c3be39452"
      },
      "source": [
        "# Adjust epochs and other parameters as needed\n",
        "# Callbacks have been commented out to avoid overwriting existin data.\n",
        "# Whoever is running this can uncomment them as needed\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=3, min_lr=0.0001),\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor='val_acc',\n",
        "        patience=5,\n",
        "        ),\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        r'C:\\Users\\cotua\\Desktop\\python scripts\\Weights\\weights.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
        "        monitor='val_acc',\n",
        "        save_best_only=True,\n",
        "        verbose=1),\n",
        "]\n",
        "\n",
        "history = model.fit_generator(\n",
        "    vid_gen,\n",
        "    steps_per_epoch=math.ceil(vid_gen.files_count/BS),\n",
        "    validation_data=validation_gen,\n",
        "    verbose=1,\n",
        "    epochs=30, # last used 80,\n",
        "    shuffle=True,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "c:\\users\\cotua\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "39/39 [==============================] - 186s 4s/step - loss: 2.6939 - acc: 0.5431 - val_loss: 1.7272 - val_acc: 0.8727\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.87273, saving model to C:\\Users\\cotua\\Desktop\\python scripts\\Weights\\weights.01-1.73.hdf5\n",
            "Epoch 2/30\n",
            "39/39 [==============================] - 32s 809ms/step - loss: 1.4729 - acc: 0.8840 - val_loss: 1.0070 - val_acc: 0.9000\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.87273 to 0.90000, saving model to C:\\Users\\cotua\\Desktop\\python scripts\\Weights\\weights.02-1.01.hdf5\n",
            "Epoch 3/30\n",
            "39/39 [==============================] - 32s 811ms/step - loss: 0.8276 - acc: 0.9412 - val_loss: 0.6577 - val_acc: 0.9182\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.90000 to 0.91818, saving model to C:\\Users\\cotua\\Desktop\\python scripts\\Weights\\weights.03-0.66.hdf5\n",
            "Epoch 4/30\n",
            "39/39 [==============================] - 32s 810ms/step - loss: 0.5225 - acc: 0.9762 - val_loss: 0.4917 - val_acc: 0.9455\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.91818 to 0.94545, saving model to C:\\Users\\cotua\\Desktop\\python scripts\\Weights\\weights.04-0.49.hdf5\n",
            "Epoch 5/30\n",
            "39/39 [==============================] - 32s 811ms/step - loss: 0.3595 - acc: 0.9814 - val_loss: 0.4792 - val_acc: 0.8818\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.94545\n",
            "Epoch 6/30\n",
            "39/39 [==============================] - 32s 813ms/step - loss: 0.2818 - acc: 0.9820 - val_loss: 0.3475 - val_acc: 0.9182\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.94545\n",
            "Epoch 7/30\n",
            "39/39 [==============================] - 32s 811ms/step - loss: 0.2103 - acc: 0.9965 - val_loss: 0.4200 - val_acc: 0.8818\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.94545\n",
            "Epoch 8/30\n",
            "39/39 [==============================] - 32s 813ms/step - loss: 0.1856 - acc: 0.9906 - val_loss: 0.3063 - val_acc: 0.9364\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.94545\n",
            "Epoch 9/30\n",
            "39/39 [==============================] - 32s 813ms/step - loss: 0.1436 - acc: 0.9987 - val_loss: 0.2727 - val_acc: 0.9273\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.94545\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_sDIimUedl1"
      },
      "source": [
        "# uncomment only if you need to\n",
        "model.save(r'C:\\Users\\cotua\\Desktop\\python scripts\\Inception_BLSTM_model_V10_lr_0.0005.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYkLneS3eXQ4"
      },
      "source": [
        "# uncomment only if you need to\n",
        "np.save(r'C:\\Users\\cotua\\Desktop\\python scripts\\train_history_Inception_model_V10_lr_0.0005.npy',history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gG8n-RzpDo5"
      },
      "source": [
        "**Testing model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dzqgMeq_yhk"
      },
      "source": [
        "# Note that this takes a lot of RAM and might crash. Hence, reduce step size to run"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_d32FKoX90v6"
      },
      "source": [
        "model=load_model(r'/content/drive/MyDrive/CSCE636/v10/Inception_BLSTM_model_V10_lr_0.0005.h5')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BfUQoELCCWw"
      },
      "source": [
        "!unzip -q \"/content/drive/MyDrive/CSCE636/v5/youtube-neg-test.zip\""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfXkwlvNCaXO"
      },
      "source": [
        "!unzip -q \"/content/drive/MyDrive/CSCE636/v10/youtube-pos-test.zip\""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8Xs4zTDpfL5"
      },
      "source": [
        "Positive Youtube Videos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKuihExs-tQp",
        "outputId": "b7acae69-e98f-4380-e426-e7487b7a01cd"
      },
      "source": [
        "test_gen = VideoFrameGenerator(\n",
        "    glob_pattern=r\"/content/youtube-pos-test/{classname}/*\",\n",
        "    nb_frames=20, \n",
        "    shuffle=False,\n",
        "    batch_size=16,\n",
        "    target_shape=img_shape,\n",
        "    nb_channel=3,\n",
        "    transformation=ImageDataGenerator(rescale=1./255,\n",
        "                                      samplewise_center=True,\n",
        "                                      # rotation_range=30,\n",
        "                                      # width_shift_range=0.1,\n",
        "                                      # height_shift_range=0.1,\n",
        "                                      # shear_range=0.1,\n",
        "                                      # zoom_range=0.1,\n",
        "                                      # horizontal_flip=True,\n",
        "                                      fill_mode=\"nearest\"),\n",
        "    use_frame_cache=False)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total data: 2 classes for 272 files for train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hlu7s3Pj3AZr",
        "outputId": "925166f1-32d6-4e8b-966b-0253689e1369"
      },
      "source": [
        "len(test_gen)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abMv3YYC3AZr",
        "outputId": "987ab370-09b3-4335-bdbe-842c01a9673b"
      },
      "source": [
        "model.evaluate_generator(test_gen, steps=len(test_gen))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "c:\\users\\cotua\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1877: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
            "  warnings.warn('`Model.evaluate_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.23335778713226318, 0.9485294222831726]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LF496IDBog5"
      },
      "source": [
        "Negative Youtube Videos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFKdOtiwB5Km",
        "outputId": "0e43e833-a112-4851-b8e7-300a4809f27b"
      },
      "source": [
        "test_gen = VideoFrameGenerator(\n",
        "    glob_pattern=r\"/content/youtube-neg-test/{classname}/*\",\n",
        "    nb_frames=20, \n",
        "    shuffle=True,\n",
        "    batch_size=31,\n",
        "    target_shape=img_shape,\n",
        "    nb_channel=3,\n",
        "    transformation=ImageDataGenerator(rescale=1./255,\n",
        "                                      samplewise_center=True,\n",
        "                                      # rotation_range=30,\n",
        "                                      # width_shift_range=0.1,\n",
        "                                      # height_shift_range=0.1,\n",
        "                                      # shear_range=0.1,\n",
        "                                      # zoom_range=0.1,\n",
        "                                      # horizontal_flip=True,\n",
        "                                      fill_mode=\"nearest\"),\n",
        "    use_frame_cache=False)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total data: 2 classes for 310 files for train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JcW4UI996c-",
        "outputId": "44045c19-30a1-4d4f-d219-dad24f69fbb2"
      },
      "source": [
        "len(test_gen)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dq_OlorkB7Lw",
        "outputId": "1d8ed5ea-fc8f-4ae2-aaf2-9f5663d263c1"
      },
      "source": [
        "model.evaluate_generator(test_gen, steps=1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1877: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
            "  warnings.warn('`Model.evaluate_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5868940949440002, 0.9032257795333862]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LnmPFHL3AZt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}