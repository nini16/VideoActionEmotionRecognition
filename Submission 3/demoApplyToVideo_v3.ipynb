{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "demoApplyToVideo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0raDBAcukRE",
        "outputId": "3cb8feea-6602-41c2-c11b-8b47f979136e"
      },
      "source": [
        "# just in case\r\n",
        "!pip install tqdm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aj0aI2iv_BLw",
        "outputId": "aed96a19-4510-4fb9-c87f-667bec1510ba"
      },
      "source": [
        "# mount google drive\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive/')\r\n",
        "# drive.flush_and_unmount()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZD4i9x70-fCx"
      },
      "source": [
        "import keras\n",
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO0loo7S-fCx"
      },
      "source": [
        "def ExtractFrames(file_path, pos=[0,0.05,0.1,0.15,.02,.25,.30,.35,.40,.45,.50,.55,.60,.65,.70,.75,.80,.85,.9,.95]):\r\n",
        "    # Extracts frames from file_path at the positions (relative between 0 and 1) in pos\r\n",
        "    \r\n",
        "    import os\r\n",
        "    \r\n",
        "    if not len(pos):\r\n",
        "        print(\"[ExtractFrames]: Invalid positions\")\r\n",
        "        return None\r\n",
        "    \r\n",
        "    if not os.path.isfile(file_path) :\r\n",
        "        print(\"[ExtractFrames]: Invalid file path\")\r\n",
        "        return None\r\n",
        "    \r\n",
        "    import cv2\r\n",
        "    \r\n",
        "    # container for frames\r\n",
        "    arr = np.empty((len(pos),224,224,3))\r\n",
        "    \r\n",
        "    cap = cv2.VideoCapture(file_path)\r\n",
        "    total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\r\n",
        "    \r\n",
        "    for k,i in enumerate(pos):\r\n",
        "        # get frame number\r\n",
        "        position = int(i * total_frames)\r\n",
        "        \r\n",
        "        # set frame pointer at i and extract frame\r\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\r\n",
        "        ret, frame = cap.read()\r\n",
        "        \r\n",
        "        # preprocessing\r\n",
        "        frame = cv2.resize(frame, (224,224))\r\n",
        "        frame = frame * 1/255.\r\n",
        "        frame = np.float32(frame)\r\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\r\n",
        "        \r\n",
        "        # insert in container\r\n",
        "        arr[k] = frame\r\n",
        "        \r\n",
        "    # cleanup\r\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\r\n",
        "    cap.release()\r\n",
        "    \r\n",
        "    return arr"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nozn0bg7-fCy"
      },
      "source": [
        "# Load our already trained model\r\n",
        "model = load_model(r\"/content/drive/MyDrive/CSCE636/v3/main_model_V3_lr_0.0005.h5\")\r\n",
        "model.load_weights(r\"/content/drive/MyDrive/CSCE636/v3/weights.07-0.38.hdf5\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4cBHp0Ru2y-"
      },
      "source": [
        "# Get frames from our desired test video\r\n",
        "TestVideo = ExtractFrames(r\"/content/drive/MyDrive/CSCE636/v3/DemoApplyTestVideo_v3.mp4\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhoJdhjru4ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31305966-bd48-483c-f4f0-1cadcba9ae0b"
      },
      "source": [
        "# need to reshape input to include sample size.\r\n",
        "# in this case sample size is 1\r\n",
        "TestVideo = np.expand_dims(TestVideo, axis=0)\r\n",
        "TestVideo.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 20, 224, 224, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtZYYAypBDAJ"
      },
      "source": [
        "# perform prediction. Note this command works for only a single video\r\n",
        "pred = model(TestVideo)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YU1_kwZGPTC_"
      },
      "source": [
        "# convert tensor to array\r\n",
        "pred = pred.numpy()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MC3C9_5QItM",
        "outputId": "feb1ee70-1810-4079-c6dd-d7a8f42b2e17"
      },
      "source": [
        "# The first index represents brusing/combing hair, the second is not brushing/combing hair\r\n",
        "pred"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.98032767, 0.01967234]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5x1VzGMBUvN"
      },
      "source": [
        "index_max = np.argmax(pred, axis=1)\r\n",
        "# \"brushing_hair\" - 1, \"not brushing_hair\" - 0\r\n",
        "# if argmax is index 0, then it predicted brushing hair, hence\r\n",
        "# assign a 1 or else assign a 0\r\n",
        "lookup = {1:0, 0:1}\r\n",
        "predicted_labels = np.array([lookup[i] for i in index_max])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c-S8DmJQaxS",
        "outputId": "11429cc9-11f9-4a1d-d8f8-4d018cd0522e"
      },
      "source": [
        "# a value of one means we detected the action!\r\n",
        "predicted_labels[0]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    }
  ]
}