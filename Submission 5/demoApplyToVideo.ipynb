{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "demoApplyToVideo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0raDBAcukRE",
        "outputId": "05d53c1d-255b-4fa2-9e5a-020be4713c36"
      },
      "source": [
        "# just in case\n",
        "!pip install tqdm"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aj0aI2iv_BLw",
        "outputId": "77a1de2a-e1a1-40fe-bf54-37ccc3b5de0c"
      },
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "# drive.flush_and_unmount()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZD4i9x70-fCx"
      },
      "source": [
        "import keras\n",
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO0loo7S-fCx"
      },
      "source": [
        "def ExtractFrames(file_path, pos=[0,0.05,0.1,0.15,.02,.25,.30,.35,.40,.45,.50,.55,.60,.65,.70,.75,.80,.85,.9,.95]):\n",
        "    # Extracts frames from file_path at the positions (relative between 0 and 1) in pos\n",
        "    \n",
        "    import os\n",
        "    \n",
        "    if not len(pos):\n",
        "        print(\"[ExtractFrames]: Invalid positions\")\n",
        "        return None\n",
        "    \n",
        "    if not os.path.isfile(file_path) :\n",
        "        print(\"[ExtractFrames]: Invalid file path\")\n",
        "        return None\n",
        "    \n",
        "    import cv2\n",
        "    \n",
        "    # container for frames\n",
        "    arr = np.empty((len(pos),224,224,3))\n",
        "    \n",
        "    cap = cv2.VideoCapture(file_path)\n",
        "    total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "    \n",
        "    for k,i in enumerate(pos):\n",
        "        # get frame number\n",
        "        position = int(i * total_frames)\n",
        "        \n",
        "        # set frame pointer at i and extract frame\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "        ret, frame = cap.read()\n",
        "        \n",
        "        # preprocessing\n",
        "        frame = cv2.resize(frame, (224,224))\n",
        "        frame = frame * 1/255.\n",
        "        frame = np.float32(frame)\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        # insert in container\n",
        "        arr[k] = frame\n",
        "        \n",
        "    # cleanup\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
        "    cap.release()\n",
        "    \n",
        "    return arr"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nozn0bg7-fCy"
      },
      "source": [
        "# Load our already trained model\n",
        "model = load_model(r\"/content/drive/MyDrive/CSCE636/v5/bidirectionalLSTM_model_V5_lr_0.0005.h5\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4cBHp0Ru2y-"
      },
      "source": [
        "# Get frames from our desired test video\n",
        "TestVideo = ExtractFrames(r\"/content/drive/MyDrive/CSCE636/v5/DemoApplyTestVideo_v5.mp4\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhoJdhjru4ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6538455-d729-442c-e177-e5afc01f9822"
      },
      "source": [
        "# need to reshape input to include sample size.\n",
        "# in this case sample size is 1\n",
        "TestVideo = np.expand_dims(TestVideo, axis=0)\n",
        "TestVideo.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 20, 224, 224, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtZYYAypBDAJ"
      },
      "source": [
        "# perform prediction. Note this command works for only a single video\n",
        "pred = model(TestVideo)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YU1_kwZGPTC_"
      },
      "source": [
        "# convert tensor to array\n",
        "pred = pred.numpy()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MC3C9_5QItM",
        "outputId": "38b17a06-6e5c-42de-8c1f-dc7be7087c0c"
      },
      "source": [
        "# The first index represents shaking hand, the second is not shaking hand\n",
        "pred"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9943394 , 0.00566066]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5x1VzGMBUvN"
      },
      "source": [
        "index_max = np.argmax(pred, axis=1)\n",
        "# \"Shaking_Hand\" - 1, \"not Shaking_Hand\" - 0\n",
        "# if argmax is index 0, then it predicted Shaking hand, hence\n",
        "# assign a 1 or else assign a 0\n",
        "lookup = {1:0, 0:1}\n",
        "predicted_labels = np.array([lookup[i] for i in index_max])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c-S8DmJQaxS",
        "outputId": "224d6640-9f83-4152-968f-c184e9614339"
      },
      "source": [
        "# a value of one means we detected the action!\n",
        "predicted_labels[0]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    }
  ]
}