{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "demoTrainAndTest_v5_corrected testing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srPHiZhPjbCF"
      },
      "source": [
        "Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9fmd4KnymJr",
        "outputId": "f71efccb-7d58-4a32-c1ea-6114ce428011"
      },
      "source": [
        "# Provides similar functionality to ImageDataGenerators for videos\n",
        "!pip install keras-video-generators"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-video-generators\n",
            "  Downloading https://files.pythonhosted.org/packages/d9/98/ff550be6084b0537f1340783a6850a2f2b62b87273472a56c17ed84bcdb3/keras-video-generators-1.0.14.tar.gz\n",
            "Requirement already satisfied: keras>=2 in /usr/local/lib/python3.7/dist-packages (from keras-video-generators) (2.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-video-generators) (1.19.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from keras-video-generators) (4.1.2.30)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from keras-video-generators) (3.2.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras>=2->keras-video-generators) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras>=2->keras-video-generators) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras>=2->keras-video-generators) (1.4.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->keras-video-generators) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->keras-video-generators) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->keras-video-generators) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->keras-video-generators) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras>=2->keras-video-generators) (1.15.0)\n",
            "Building wheels for collected packages: keras-video-generators\n",
            "  Building wheel for keras-video-generators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-video-generators: filename=keras_video_generators-1.0.14-cp37-none-any.whl size=12884 sha256=b4630d319001343701582655442303c997a1d63fe632a147dabae5d4916153bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/20/b7/76/8674d46fc4777c09e5aa7b065d4e356d90f12ec409a6144bbb\n",
            "Successfully built keras-video-generators\n",
            "Installing collected packages: keras-video-generators\n",
            "Successfully installed keras-video-generators-1.0.14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2nHw4R7yp3C",
        "outputId": "51579dbb-c663-466a-a44b-78761a419f6b"
      },
      "source": [
        "# Please email me at nini16@tamu.edu if you do not ave access to the google drive.\n",
        "# Permissions should have been granted but if not please email me!\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "# drive.flush_and_unmount()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UA_Vh5HTzPjH"
      },
      "source": [
        "# training data.\n",
        "# Please ensure the file is present before running.\n",
        "!unzip -q \"/content/drive/MyDrive/CSCE636/v5/dataset.zip\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrBsbdoWyXgq"
      },
      "source": [
        "import keras\n",
        "from keras.regularizers import l2\n",
        "from keras.preprocessing.image import load_img\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from keras.layers import Conv2D, BatchNormalization, \\\n",
        "    MaxPool2D, GlobalMaxPool2D, Dense, Dropout, Bidirectional\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras_video import VideoFrameGenerator\n",
        "\n",
        "from keras.layers import TimeDistributed, GRU, Dense, Dropout, LSTM\n",
        "\n",
        "from keras.models import load_model, Model\n",
        "\n",
        "from keras.applications import ResNet50V2, DenseNet121\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import math"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7Xx0DXOzJ30"
      },
      "source": [
        "# All frames should be resized\n",
        "# Please select a batch-size that divides the number of samples!\n",
        "# THE DATASET WILL PROBABLY BE INCREASED FOR THE NEXT SUBMISSION SO\n",
        "# BE SURE TO ADJUST THE BATCH SIZE!!\n",
        "\n",
        "img_shape = (224, 224)\n",
        "BS = 32"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kS-o0BZzJ7v",
        "outputId": "678e3ca1-6c01-48d0-a363-9325203b5857"
      },
      "source": [
        "# Apply image augmentation to each frame\n",
        "# Please confirm that this directory is present before running\n",
        "\n",
        "vid_gen = VideoFrameGenerator(\n",
        "    glob_pattern=r\"/content/dataset/train/{classname}/*\",\n",
        "    nb_frames=20, \n",
        "    shuffle=True,\n",
        "    batch_size=BS,\n",
        "    target_shape=img_shape,\n",
        "    nb_channel=3,\n",
        "    transformation=ImageDataGenerator(rescale=1./255,\n",
        "                                      samplewise_center=True,\n",
        "                                      # rotation_range=30,\n",
        "                                      # width_shift_range=0.1,\n",
        "                                      # height_shift_range=0.1,\n",
        "                                      # shear_range=0.1,\n",
        "                                      # zoom_range=0.1,\n",
        "                                      # horizontal_flip=True,\n",
        "                                      fill_mode=\"nearest\"),\n",
        "    use_frame_cache=False)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total data: 2 classes for 1024 files for train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqbQiJKp784D",
        "outputId": "46bf6e5d-d9f2-4a11-fdbf-99dced614e2e"
      },
      "source": [
        "validation_gen = VideoFrameGenerator(\n",
        "    glob_pattern=r\"/content/dataset/validation/{classname}/*\",\n",
        "    nb_frames=20, \n",
        "    shuffle=True,\n",
        "    batch_size=BS,\n",
        "    target_shape=img_shape,\n",
        "    nb_channel=3,\n",
        "    transformation=ImageDataGenerator(rescale=1./255,\n",
        "                                      samplewise_center=True,\n",
        "                                      # rotation_range=30,\n",
        "                                      # width_shift_range=0.1,\n",
        "                                      # height_shift_range=0.1,\n",
        "                                      # shear_range=0.1,\n",
        "                                      # zoom_range=0.1,\n",
        "                                      # horizontal_flip=True,\n",
        "                                      fill_mode=\"nearest\"),\n",
        "    use_frame_cache=False)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total data: 2 classes for 64 files for train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxiE9U_YzJ_2"
      },
      "source": [
        "# model structure for Feature Extractor\n",
        "def build_convnet_3(shape=(224, 224, 3)):\n",
        "    resnet = DenseNet121(include_top=False, weights='imagenet', input_shape=shape)\n",
        "\n",
        "    train = False\n",
        "    for layer in resnet.layers:\n",
        "        layer.trainable = train\n",
        "        if layer.name == \"conv5_block3_2_relu\":\n",
        "            train = True\n",
        "    \n",
        "    globMaxpool = GlobalMaxPool2D()(resnet.output)\n",
        "    model = Model(inputs=resnet.input, outputs=globMaxpool)\n",
        "    return model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RlyaxyBzMf6"
      },
      "source": [
        "def action_model(shape=(20,) + img_shape + (3,)):\n",
        "    # Create our feature extractor convnet with img_shape input shape\n",
        "    convnet = build_convnet_3()\n",
        "    \n",
        "    # then create our final model\n",
        "    model = keras.Sequential()\n",
        "    # add the convnet with img_shape shape\n",
        "    model.add(TimeDistributed(convnet, input_shape=shape))\n",
        "    # add GRU\n",
        "    model.add(Bidirectional(LSTM(64)))\n",
        "    # and finally, we make a decision network\n",
        "    model.add(Dense(1024, activation='relu', kernel_regularizer=keras.regularizers.l2(l2=0.01)))\n",
        "    model.add(Dropout(.5))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcVhpRcs3iLk",
        "outputId": "76c5f51f-409c-4d4b-95e5-ea55c094b1a1"
      },
      "source": [
        "# instantiate and compile model\n",
        "model = action_model()\n",
        "optimizer = keras.optimizers.Adam(0.0005) #0.0005\n",
        "model.compile(\n",
        "    optimizer,\n",
        "    'categorical_crossentropy',\n",
        "    metrics=['acc']\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "29089792/29084464 [==============================] - 0s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "time_distributed (TimeDistri (None, 20, 1024)          7037504   \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 128)               557568    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1024)              132096    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 2050      \n",
            "=================================================================\n",
            "Total params: 7,729,218\n",
            "Trainable params: 691,714\n",
            "Non-trainable params: 7,037,504\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjuNkqG47uXR",
        "scrolled": true,
        "outputId": "e3a8a33f-2bdc-40e6-ebfe-0e7c3be39452"
      },
      "source": [
        "# Adjust epochs and other parameters as needed\n",
        "# Callbacks have been commented out to avoid overwriting existin data.\n",
        "# Whoever is running this can uncomment them as needed\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=3, min_lr=0.0001),\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor='val_acc',\n",
        "        patience=5,\n",
        "        ),\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        r'C:\\Users\\cotua\\Desktop\\python scripts\\Weights\\weights.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
        "        monitor='val_acc',\n",
        "        save_best_only=True,\n",
        "        verbose=1),\n",
        "]\n",
        "\n",
        "history = model.fit_generator(\n",
        "    vid_gen,\n",
        "    steps_per_epoch=math.ceil(vid_gen.files_count/BS),\n",
        "    validation_data=validation_gen,\n",
        "    verbose=1,\n",
        "    epochs=30, # last used 80,\n",
        "    shuffle=True,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "32/32 [==============================] - 65s 2s/step - loss: 1.7715 - acc: 0.5448 - val_loss: 1.3765 - val_acc: 0.6250\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.62500, saving model to C:\\Users\\cotua\\Desktop\\python scripts\\Weights\\weights.01-1.38.hdf5\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 54s 2s/step - loss: 1.2330 - acc: 0.7926 - val_loss: 0.9932 - val_acc: 0.7656\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.62500 to 0.76562, saving model to C:\\Users\\cotua\\Desktop\\python scripts\\Weights\\weights.02-0.99.hdf5\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 54s 2s/step - loss: 0.9250 - acc: 0.7936 - val_loss: 0.7933 - val_acc: 0.7812\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.76562 to 0.78125, saving model to C:\\Users\\cotua\\Desktop\\python scripts\\Weights\\weights.03-0.79.hdf5\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 54s 2s/step - loss: 0.6926 - acc: 0.8480 - val_loss: 0.6655 - val_acc: 0.8125\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.78125 to 0.81250, saving model to C:\\Users\\cotua\\Desktop\\python scripts\\Weights\\weights.04-0.67.hdf5\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 54s 2s/step - loss: 0.5060 - acc: 0.9049 - val_loss: 0.5562 - val_acc: 0.8750\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.81250 to 0.87500, saving model to C:\\Users\\cotua\\Desktop\\python scripts\\Weights\\weights.05-0.56.hdf5\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 54s 2s/step - loss: 0.4088 - acc: 0.9317 - val_loss: 0.5754 - val_acc: 0.7500\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.87500\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 54s 2s/step - loss: 0.3947 - acc: 0.9062 - val_loss: 0.5328 - val_acc: 0.8125\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.87500\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 54s 2s/step - loss: 0.3129 - acc: 0.9548 - val_loss: 0.4600 - val_acc: 0.8594\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.87500\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 54s 2s/step - loss: 0.2627 - acc: 0.9711 - val_loss: 0.3750 - val_acc: 0.9062\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.87500 to 0.90625, saving model to C:\\Users\\cotua\\Desktop\\python scripts\\Weights\\weights.09-0.37.hdf5\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 54s 2s/step - loss: 0.2126 - acc: 0.9731 - val_loss: 0.4370 - val_acc: 0.8594\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.90625\n",
            "Epoch 11/30\n",
            "32/32 [==============================] - 54s 2s/step - loss: 0.1675 - acc: 0.9929 - val_loss: 0.3912 - val_acc: 0.8750\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.90625\n",
            "Epoch 12/30\n",
            "32/32 [==============================] - 54s 2s/step - loss: 0.1495 - acc: 0.9942 - val_loss: 0.3666 - val_acc: 0.8594\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.90625\n",
            "Epoch 13/30\n",
            "32/32 [==============================] - 54s 2s/step - loss: 0.1188 - acc: 0.9975 - val_loss: 0.3687 - val_acc: 0.8438\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.90625\n",
            "Epoch 14/30\n",
            "32/32 [==============================] - 54s 2s/step - loss: 0.1088 - acc: 0.9966 - val_loss: 0.3812 - val_acc: 0.9062\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.90625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_sDIimUedl1"
      },
      "source": [
        "# uncomment only if you need to\n",
        "model.save(r'C:\\Users\\cotua\\Desktop\\python scripts\\new_model_V5_lr_0.0005.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYkLneS3eXQ4"
      },
      "source": [
        "# uncomment only if you need to\n",
        "np.save(r'C:\\Users\\cotua\\Desktop\\python scripts\\train_history_new_model_V5_lr_0.0005.npy',history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gG8n-RzpDo5"
      },
      "source": [
        "**Testing model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_d32FKoX90v6"
      },
      "source": [
        "model=load_model(r'/content/drive/MyDrive/CSCE636/v5/bidirectionalLSTM_model_V5_lr_0.0005.h5')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BfUQoELCCWw"
      },
      "source": [
        "!unzip -q \"/content/drive/MyDrive/CSCE636/v5/youtube-neg-test.zip\""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfXkwlvNCaXO"
      },
      "source": [
        "!unzip -q \"/content/drive/MyDrive/CSCE636/v5/youtube_positive.zip\""
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8Xs4zTDpfL5"
      },
      "source": [
        "Positive Youtube Videos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKuihExs-tQp",
        "outputId": "ba107384-5100-4f8e-b4af-5c4210211c0a"
      },
      "source": [
        "test_gen = VideoFrameGenerator(\n",
        "    glob_pattern=r\"/content/youtube_positive/{classname}/*\",\n",
        "    nb_frames=20, \n",
        "    shuffle=True,\n",
        "    batch_size=13,\n",
        "    target_shape=img_shape,\n",
        "    nb_channel=3,\n",
        "    transformation=ImageDataGenerator(rescale=1./255,\n",
        "                                      samplewise_center=True,\n",
        "                                      # rotation_range=30,\n",
        "                                      # width_shift_range=0.1,\n",
        "                                      # height_shift_range=0.1,\n",
        "                                      # shear_range=0.1,\n",
        "                                      # zoom_range=0.1,\n",
        "                                      # horizontal_flip=True,\n",
        "                                      fill_mode=\"nearest\"),\n",
        "    use_frame_cache=False)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total data: 2 classes for 312 files for train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ApYRWhT90v7",
        "outputId": "d64fe3c6-0a7d-474b-ec89-6f95e73cee9c"
      },
      "source": [
        "model.evaluate_generator(test_gen, steps=len(test_gen))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3357178270816803, 0.9042553305625916]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LF496IDBog5"
      },
      "source": [
        "Negative Youtube Videos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFKdOtiwB5Km",
        "outputId": "8d54317a-e58c-4c13-c43e-99a701d4e10c"
      },
      "source": [
        "test_gen = VideoFrameGenerator(\n",
        "    glob_pattern=r\"/content/youtube-neg-test/{classname}/*\",\n",
        "    nb_frames=20, \n",
        "    shuffle=True,\n",
        "    batch_size=10,\n",
        "    target_shape=img_shape,\n",
        "    nb_channel=3,\n",
        "    transformation=ImageDataGenerator(rescale=1./255,\n",
        "                                      samplewise_center=True,\n",
        "                                      # rotation_range=30,\n",
        "                                      # width_shift_range=0.1,\n",
        "                                      # height_shift_range=0.1,\n",
        "                                      # shear_range=0.1,\n",
        "                                      # zoom_range=0.1,\n",
        "                                      # horizontal_flip=True,\n",
        "                                      fill_mode=\"nearest\"),\n",
        "    use_frame_cache=False)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total data: 2 classes for 310 files for train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JcW4UI996c-",
        "outputId": "44045c19-30a1-4d4f-d219-dad24f69fbb2"
      },
      "source": [
        "len(test_gen)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dq_OlorkB7Lw",
        "outputId": "85d47dc7-b56e-43cc-f2c0-c3fbd590d87d"
      },
      "source": [
        "model.evaluate_generator(test_gen, steps=len(test_gen))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1877: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
            "  warnings.warn('`Model.evaluate_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9120622277259827, 0.725806474685669]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    }
  ]
}